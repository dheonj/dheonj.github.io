---
gㅜ후title: "파이썬을 이용한 트위터 크롤링 방법들"
date: 2020-07-20T22:42:30+09:00
draft: true
---

이전까지 했던 코드 보다보니 정리할 필요를 느껴서 ㅇㅇ

트위터 주소가 있을때 그 트윗카드의 컨텐츠, 작성자 등을 크롤링하는 코드를 짰음. 몇가지 방법이 있는게 간단하게 정리해본다.



1. chromedriver를 사용하는 방법 (비추)

chromedriver는 크롤링을 위해 크롬브라우저를 자동으로 으쌰으쌰해주는 프로그램으로,

셀레늄(크롬드라이버를 사용하는 파이썬 크롤링 라이브러리)이라고 검색하면 기본적인 사용법은 나옴.

브라우저를 키고, 트위터 주소에 접속한 후 미리 지정한 element를 찾아가 정보를 긁어오는 방법인데, 실제로 처음 돌려보면 브라우저가 자동으로 켜지고 url에 접속도 되고 해서 보는 재미가 있다. 아래는 내가 작성한 코드.

```python
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
import sys
import io
from selenium.webdriver.common.action_chains import ActionChains

#글자가 깨지는 경우가 있어서 추가한것같은데, 정확히 기억안남.
sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding = 'utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding = 'utf-8')

#target tweet html address
tweet_list=[
"https://twitter.com/SpaceX/status/1280624175590240256",
"https://twitter.com/elonmusk/status/1279867930289647617"
]

#open chromedriver
path ="C:\\Users\\user\\Documents\\github\\chromedriver.exe" #chromedriver 위치
driver=webdriver.Chrome(path)

for tweet in tweet_list:
    driver.get(tweet)
    print("page address: " + tweet)
    # wait until page is loaded
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "article")))
        ActionChains(driver).send_keys(Keys.ESCAPE).perform()
    except:
        print("Unexpected error:????", sys.exc_info()[0])
        raise

 tweet=driver.find_element_by_xpath("""/html/body/div/div/div/div[2]/main/div/div/div/div/div/div/div/section/div/div/div/div[1]/div/div/article/div/div/div/div[3]/div[1]/div/span""")
    nickname=driver.find_element_by_xpath("/html/body/div/div/div/div[2]/main/div/div/div/div/div/div/div/section/div/div/div/div[1]/div/div/article/div/div/div/div[2]/div[2]/div/div/div/div[1]/a/div/div[1]/div[1]/span/span")
    account=driver.find_element_by_xpath("/html/body/div/div/div/div[2]/main/div/div/div/div/div/div/div/section/div/div/div/div[1]/div/div/article/div/div/div/div[2]/div[2]/div/div/div/div[1]/a/div/div[2]/div/span")
    # id=driver.find_element_by_xpath(".//div/div[2]/div/span")
    print("NICKNAME: "+nickname.text+"  ID: " + account.text)
    # article=tweet.find_elements_by_xpath(""".//div/div[3]/div""")
    print("TEXT: "+tweet.text)
```

그러나 몇가지 문제가 있다.

element의 클래스 이름이나 xpath를 지정해서 찾아갈 수 있지만, 트위터의 경우는 element의 클래스 이름이 랜덤하게 바뀌고 쓰레드에 따라 xpath가 여러 경우라서 오류가 발생하더라.

또한 브라우저상에서 작동하는 것이라 기본적으로 크롤링 속도가 느리다.



2. request를 사용하는 방법

위에서 말한 것처럼 브라우저는 서버에서 보내준 html소스를 렌더링해서 보여주는 역할을 하는데(다른 역할도 있겠지만), 렌더링을 하지 않고 html소스에서 필요한 정보만 뽑아내면 속도가 증가할 것이다. 

```python
from bs4 import BeautifulSoup
from ast import literal_eval
import requests
import json
import certifi
import urllib3
from urllib.parse import urlparse

targeturl='https://t.co/aVnwR0loX9'

http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())
headers={'User-Agent': "NaverBot"}

html = requests.get(targeturl, headers=headers)
soup = BeautifulSoup(html.text, 'html.parser')
title = soup.find('meta', property='og:title')
description = soup.find('meta', property='og:description')
image = soup.find('meta', property='og:image')

return_data = {}
return_data['title'] = title['content']
return_data['content'] = description['content']
if image != None:
    return_data['imgsrc'] = image['content']
else:
    return_data['imgsrc'] = ''
return_json = json.dumps(return_data, ensure_ascii=False)
print(return_json)
```

<s>문제는 사이트에 따라서 robot이 보내는 request가 막혀있을 수 있다.  대표적으로 instagram. 머 요청을 보낼때 클라이언트 정보를 인위적으로 바꿔서 해도 되지만, 기본적으로 cheating이라고 생각한다. </s> 

> 크롤링 할 사이트들 중 인스타그램에서만 request가 작동을 안해서 엄청나게 삽질을 많이 했다. user-agent문제라고 생각해서 googlebot부터 무슨 노키아 폰으로 접속할때 쓰는 에이전트ㅋㅋㅋㅋ 그런것까지 다  써봤는데, 결론은 인스타에서 구글 클라우드 서버 ip를 블럭한 듯. 스택오버플로에도 aws나 구글 스프레드시트에서 인스타 정보에 접근할 수 없다는 (로그인을 하지 않으면) 질문이 몇 개 있다.

아무튼,, 이건 내가 gcf를 사용해서 발생한 문제지만, 사이트에 따라서는 정식 api를 쓰지 않으면 크롤링이 불가능한 경우가 종종 있음.

그래서 트위터가 제공하는 api를 쓰자 이거야.



3. 트위터 api 사용

```python
import tweepy
from bs4 import BeautifulSoup
from ast import literal_eval
import requests
import json
from urllib.parse import quote

#트위터에서 생성한 api 접근 키
consumer_key='~~~~'
consumer_secret='~~~~'
access_token_key='~~~~'
access_token_secret='~~~~' 

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token_key, access_token_secret)
api = tweepy.API(auth)

oembed='https://publish.twitter.com/oembed?url='
target='https://twitter.com/business/status/1261999133797335040'

#방법 1.api 사용. 
#해당 post의 id를 이용 (게시물마다 고유의 id가 할당됨. target url에서 status / 뒷부분)
usingAPI=api.get_status(1261999133797335040,tweet_mode="extended")
#print(usingAPI.full_text)
#print(usingAPI.entities)
if usingAPI.entities.get('media')!=None:
    print(usingAPI.entities.get('media')[0].get('media_url'))
tweet2=api.get_oembed(url=target)
print(tweet2)

#방법 2. oembed를 이용
#전체 url을 넣어주면 됨.
response=requests.get(oembed+target)
html=response.text
#print(html)
tweet_json=(json.loads(html))
#print(a)
tweet_html=tweet_json['html']
soup=BeautifulSoup(tweet_html, 'html.parser')
print(soup)
tweet=soup.find('p')
print(tweet)
```

음 근데

oembed도 좋은 방법인 것 같다 

모든 사이트에 보편적인 것 같진 않지만,,



일단 지금까지의 으쌰으쌰를 바탕으로 한 크롤링 전략

1. meta 태그 퍼오기
2. (1이 안되면) oembed 이용
3. (2이 안되면) chromedriver이용해서 실제로 웹사이트를 열고 크롤링. (그때는 앞서 설명한것처럼 xpath등을 이용하지 말고 브라우저가 받아온 html을 파싱)



